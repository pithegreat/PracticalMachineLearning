---
title: "Predicting the Intensity of an Exercise Using Machine Learning"
author: "Tejas Guha"
date: "July 30, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Introduction
Everyday, fitness trackers such as *Fitbit* or *Jawbone* track the quantity of activity.  However, they do not track the *quality* of exercise done, which is equally as important as quantity. The purpose of the developed algorithm is to predict the intensity of an exercise done by a participant.   The machine learning algorithm uses training data and testing data collected from <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv> and <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>.In the data set, this intensity is represented by the qualitative variable, *Classe*, which has 5 levels, A being the most intense, and E being the least.  Data was collected from sensors on the participants' hips, waist, and arms, and the dumbell being lifted.  

##Analyzing the Data
Opening the training set we see that the data contains 160 features.  Many of these features only contain missing or invalid observations.
Additionally, the data contains features that should not be used in prediction, such as the user, and features irrelevant to prediction such as the window number.  These features are removed.

```{r message=FALSE, warning=FALSE}
library(caret)
training = read.csv("pml-training.csv", na.strings=c('#DIV/0', '', 'NA'))
training = training[,8:160]
testing = read.csv("pml-testing.csv", na.strings=c('#DIV/0', ' ', 'NA'))
testing = testing[,8:160]
training = training[,apply(training,2, function(x) !anyNA(x))]
testing = testing[,apply(testing,2, function(x) !anyNA(x))]
head(training[,1:5])
```

With now 53 features, the training set still has too many variables that would make any model run inefficiently.  To remove the highly correlated variables, and reduce the number of variables, a PCA analysis was applied to the training set.  The graph shows the distribution of the different classes, on the graph of the PC1 and PC2 set.  The red lines indicate the relation a feature has to PC1 and PC2.

```{r message=FALSE, warning=FALSE}
library(ggbiplot)
pr= prcomp(training[,-53], center=TRUE, scale=TRUE)
g = ggbiplot(pr, obs.scale = 1, var.scale=1, ellipse=TRUE, circle=TRUE, groups = training$classe)
g = g + theme(legend.direction = 'horizontal', legend.position='top')
print(g)
```

##Building and Testing the Model
Other models were analyzed, such as a boosted LDA model, however, the accuracy of the random forest model was unmatched in this situtation.  The random forest model was much faster with the randomForest package than the caret package, and computationally less stressing than the boosted LDA model.  To test the random forest model, a 10-fold cross validation was done, with the accuracy and statistics of each model recorded in the data frame **stats**.  Each fold in the cross validation was 1/10 of the size of the training set.  The PCA analysis made the randomForest model much faster.  The summary of the statistics of the model are displayed.
```{r message=FALSE, warning=FALSE}

library(randomForest)



preObj = preProcess(training[,-53],method=c("center","scale","pca"))
pca_data = predict(preObj, training[,-53])
set.seed(123)
confusion_matrices = list()
for (k in 1:10){
      set.seed(k)
      fold = unlist(createDataPartition(training$classe, p=0.1))
      fold_training = pca_data[-fold,]
      rf_fold_model = randomForest(training[-fold,]$classe ~ . , data=fold_training, ntree=100)
      pca_fold = na.omit(pca_data[fold,])
      rf_fold_predictions = predict(rf_fold_model, pca_fold)
      confusion = confusionMatrix(rf_fold_predictions, training[fold,]$classe)
      confusion_matrices[[k]] = confusion
}

header = names(confusion_matrices[[1]][[3]])
stats = apply(data.frame(rbind(lapply(header, function(x) lapply(confusion_matrices, function(y) y[[3]][[x]])))),2,unlist)
colnames(stats) = header
summary(stats)
```

## Results and Conclusion
Looking at the median and mean of accurarcies of the random forest model from the 10-fold cross validation, it can be seen that the in-sample accuracy is about 98%, better than the 89% of the LDA boosted model.  After using the random forest model on the testing set, the out-of-sample accuracy of the model is 90%.  The accuracies of the model can be improved, but with a high computational tradeoff.  Examples of a very accurate model would be a stacked boosted LDA model and random forest.  This is a very slow model, and would be impractical in real use.